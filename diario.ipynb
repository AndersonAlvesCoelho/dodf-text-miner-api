{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5265e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbc7a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\a-a-c\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\a-a-c\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import fitz\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Baixar recursos necessários do NLTK\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"rslp\")\n",
    "\n",
    "stemmer = RSLPStemmer()\n",
    "\n",
    "stop_words = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "# Carrega o modelo de linguagem do spaCy para português\n",
    "try:\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "except OSError:\n",
    "    # Se o download falhar na célula de instalação, tenta novamente\n",
    "    !python -m spacy download pt_core_news_sm\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6b2233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF salvo como: DODF 220 19-11-2025 INTEGRA.pdf\n"
     ]
    }
   ],
   "source": [
    "url = \"https://dodf.df.gov.br/dodf/jornal/visualizar-pdf?pasta=2025|11_Novembro|DODF%20220%2019-11-2025|&arquivo=DODF%20220%2019-11-2025%20INTEGRA.pdf\"\n",
    "\n",
    "raw_name = url.split(\"arquivo=\")[-1]\n",
    "file_name = raw_name.replace(\"%20\", \" \")\n",
    "\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "with open(file_name, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(\"PDF salvo como:\", file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a29d0",
   "metadata": {},
   "source": [
    "### Extração de Texto do PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21add3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open('DODF 220 19-11-2025 INTEGRA.pdf')\n",
    "texto_completo = []\n",
    "\n",
    "for pagina in doc:\n",
    "    # Extrai blocos de texto com coordenadas\n",
    "    blocos = pagina.get_text(\"blocks\")\n",
    "    \n",
    "    # Ordena por posição (top-left, depois top-right)\n",
    "    blocos_ordenados = sorted(blocos, key=lambda b: (b[1], b[0]))\n",
    "    \n",
    "    texto_pagina = \"\\n\".join([bloco[4] for bloco in blocos_ordenados if bloco[6] == 0]) \n",
    "    texto_completo.append(texto_pagina)\n",
    "\n",
    "doc.close()\n",
    "pdf_content = \"\".join(texto_completo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f619d5cc",
   "metadata": {},
   "source": [
    "### Extração de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf7ad423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- EXTRAÇÃO DAS SEÇÕES PRINCIPAIS (I, II, III) ----\n",
    "\n",
    "def extract_main_sections(text):\n",
    "    pattern = r\"(SEÇÃO\\s+[IVX]+)\"\n",
    "    partes = re.split(pattern, pdf_content)\n",
    "    extract_sections = []\n",
    "    for i in range(1, len(partes), 2):\n",
    "        titulo = partes[i].strip()\n",
    "        conteudo = partes[i+1].strip() if i+1 < len(partes) else \"\"\n",
    "        # --- FILTROS IMPORTANTES ---\n",
    "        \n",
    "        # remover seções muito pequenas (sumário/capa)\n",
    "        if len(conteudo) < 500:\n",
    "            continue\n",
    "        # ignorar sumário\n",
    "        if conteudo[:50].upper().startswith(\"SUMÁRIO\"):\n",
    "            continue\n",
    "        \n",
    "        # evitar duplicações vazias ou irrelevantes\n",
    "        if \"PAG.\" in conteudo[:80]:\n",
    "            continue\n",
    "        # manter apenas SEÇÃO I, II e III\n",
    "        if titulo not in [\"SEÇÃO I\", \"SEÇÃO II\", \"SEÇÃO III\"]:\n",
    "            continue\n",
    "        extract_sections.append({\n",
    "            \"secao\": titulo,\n",
    "            \"conteudo\": conteudo\n",
    "        })\n",
    "\n",
    "    return extract_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f0f4f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370725\n"
     ]
    }
   ],
   "source": [
    "sections = extract_main_sections(pdf_content)\n",
    "# print(json.dumps(sections, ensure_ascii=False, indent=2))\n",
    "\n",
    "section_I = sections[0][\"conteudo\"]\n",
    "section_II = sections[1][\"conteudo\"]\n",
    "section_III = sections[2][\"conteudo\"]\n",
    "print(len(section_III))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ccbf7f",
   "metadata": {},
   "source": [
    "### Seção 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4ba9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGEX DE TÍTULO DE ATO\n",
    "TITLE_REGEX = r'^[A-ZÁÉÍÓÚÃÕÂÊÔÇ ]{2,}\\s+N[º°oO\\.]?\\s*[\\d\\.\\/-]+(?:,\\s*DE\\s+[^\\n]+)?'\n",
    "TITLE_PATTERN = re.compile(TITLE_REGEX)\n",
    "\n",
    "# REGEX PARA META-INFO\n",
    "META_REGEX = re.compile(\n",
    "    r'^(?P<tipo>[A-ZÁÉÍÓÚÃÕÂÊÔÇ ]+)\\s+N[º°oO\\.]?\\s*(?P<numero>[\\d\\.\\/-]+)'\n",
    "    r'(?:,\\s*DE\\s*(?P<data>[^\\n]+))?',\n",
    "    re.UNICODE\n",
    ")\n",
    "\n",
    "# EXTRAÇÃO DO ÓRGÃO EMISSOR\n",
    "ORG_REGEX = re.compile(\n",
    "    r'\\b(GABINETE|SECRETARIA|MINISTÉRIO|SUPERINTENDÊNCIA|PREFEITURA|GOVERNO|CÂMARA|ASSEMBLEIA|TRIBUNAL|PROCURADORIA)[^\\n]{0,80}',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# FUNÇÃO PRINCIPAL\n",
    "def extract_acts_with_metadata(section_text):\n",
    "    lines = section_text.split(\"\\n\")\n",
    "\n",
    "    acts = []\n",
    "    current_title = None\n",
    "    current_content = []\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # Detectou título?\n",
    "        if TITLE_PATTERN.match(stripped):\n",
    "            if current_title:  \n",
    "                acts.append(process_act(current_title, current_content))\n",
    "\n",
    "            current_title = stripped\n",
    "            current_content = []\n",
    "            continue\n",
    "\n",
    "        # Acumula conteúdo\n",
    "        if current_title:\n",
    "            current_content.append(line)\n",
    "\n",
    "    # salva o último\n",
    "    if current_title:\n",
    "        acts.append(process_act(current_title, current_content))\n",
    "\n",
    "    return acts\n",
    "\n",
    "\n",
    "# PROCESSAR ATO INDIVIDUAL\n",
    "def process_act(title, content_lines):\n",
    "    meta = META_REGEX.match(title)\n",
    "\n",
    "    tipo = meta.group(\"tipo\").strip() if meta else None\n",
    "    numero = meta.group(\"numero\").strip() if meta else None\n",
    "    data_raw = meta.group(\"data\").strip() if meta and meta.group(\"data\") else None\n",
    "\n",
    "    # extrair ano da data\n",
    "    ano = None\n",
    "    if data_raw:\n",
    "        ano_match = re.search(r'\\b(19|20)\\d{2}\\b', data_raw)\n",
    "        if ano_match:\n",
    "            ano = int(ano_match.group(0))\n",
    "\n",
    "    # detectar órgão emissor no conteúdo\n",
    "    content_text = \"\\n\".join(content_lines).strip()\n",
    "    org_match = ORG_REGEX.search(content_text)\n",
    "    orgao = org_match.group(0).strip() if org_match else None\n",
    "\n",
    "    return {\n",
    "        \"tipo\": tipo,\n",
    "        \"numero\": numero,\n",
    "        \"data\": data_raw,\n",
    "        \"ano\": ano,\n",
    "        \"orgao\": orgao,\n",
    "        \"titulo\": title,\n",
    "        \"conteudo\": content_text\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eea4acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = extract_acts_with_metadata(section_I)\n",
    "# print(json.dumps(acts, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e111a18",
   "metadata": {},
   "source": [
    "### Seção 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "262770b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = section_II  # sua variável com todo o texto da seção II\n",
    "\n",
    "verbs_list = [\n",
    "    \"NOMEAR\",\"EXONERAR A PEDIDO\",\"EXONERAR\",\"DESIGNAR\",\n",
    "    \"DISPENSAR\",\"REMOVER\",\"RECONDUZIR\",\"RETIFICAR\",\n",
    "    \"TORNAR SEM EFEITO\",\"CONCEDER\",\"PRORROGAR\",\"CEDER\"\n",
    "]\n",
    "verbs_regex = \"|\".join(sorted(verbs_list, key=lambda s: -len(s)))\n",
    "\n",
    "# normalização leve\n",
    "t = re.sub(r\"\\r\\n?\", \"\\n\", text)\n",
    "t = re.sub(r\"\\s+\", \" \", t)\n",
    "t = re.sub(rf\"(?i)\\b({verbs_regex})\\b\", lambda m: \"\\n\\n\" + m.group(1).upper(), t)\n",
    "\n",
    "# split em blocos por ato\n",
    "blocks = [b.strip() for b in re.split(r\"\\n\\s*\\n\", t) if b.strip()]\n",
    "\n",
    "# -----------------------------\n",
    "# REGEX CORRIGIDAS\n",
    "# -----------------------------\n",
    "\n",
    "# Nome em maiúsculas\n",
    "name_re = re.compile(\n",
    "    r\"([A-ZÁÉÍÓÚÂÊÔÃÕÇ][A-ZÁÉÍÓÚÂÊÔÃÕÇ\\.\\-']+(?:\\s+[A-ZÁÉÍÓÚÂÊÔÃÕÇ\\.\\-']+){1,6})\"\n",
    ")\n",
    "\n",
    "# Matrícula\n",
    "mat_re = re.compile(\n",
    "    r\"matr[ií]cula[:\\s]*([\\d\\.\\-Xx]+)\", flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# SIGRH\n",
    "sigrh_re = re.compile(\n",
    "    r\"SIGRH[:\\s]*([0-9]{4,12})\", flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Símbolo (corrigido)\n",
    "sim_re = re.compile(\n",
    "    r\"(?:Símbolo|SIMBOLO|SÍMBOLO)[:\\s]*([A-Z]{1,4}[-\\s]?\\d{1,3})\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Cargo genérico\n",
    "cargo_re_1 = re.compile(\n",
    "    r\"Cargo(?: Público)?(?: em)?(?: de)?(?: Natureza Especial|[^,;.\\n]{1,80})\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Função (corrigido — fechamento de parêntesis validado)\n",
    "role_re = re.compile(\n",
    "    r\"\\b(?:de\\s+)?([A-ZÁÉÍÓÚÂÊÔÃÕÇ][a-záéíóúâêôãõç]+(?:\\s+[A-ZÁÉÍÓÚÂÊÔÃÕÇa-záéíóúâêôãõç]+){0,5})\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Órgãos\n",
    "org_keywords = [\n",
    "    \"Secretaria\",\"Subsecretaria\",\"Diretoria\",\"Gerência\",\"Gerencia\",\n",
    "    \"Coordenação\",\"Coordenação\",\"Gabinete\",\"Unidade\",\"Conselho\",\n",
    "    \"Administração Regional\"\n",
    "]\n",
    "\n",
    "org_re = re.compile(\n",
    "    r\"(?:(?:da|do|das|dos)\\s+)?(\"\n",
    "    r\"(?:\" + r\"|\".join(org_keywords) + r\")[^\\.\\,]{0,120})\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# FUNÇÃO PARA NORMALIZAR NOME\n",
    "# -----------------------------\n",
    "\n",
    "def title_case_name(s):\n",
    "    s = s.strip()\n",
    "    s = unicodedata.normalize('NFKD', s)\n",
    "    parts = s.split()\n",
    "    small = {\"da\",\"de\",\"do\",\"das\",\"dos\",\"e\",\"da'\", \"do'\"}\n",
    "    out = []\n",
    "    for i,p in enumerate(parts):\n",
    "        pl = p.lower()\n",
    "        if pl in small and i != 0:\n",
    "            out.append(pl)\n",
    "        else:\n",
    "            out.append(p.capitalize())\n",
    "    return \" \".join(out).replace(\"  \", \" \").strip()\n",
    "\n",
    "# -----------------------------\n",
    "# PROCESSAMENTO\n",
    "# -----------------------------\n",
    "\n",
    "entries = []\n",
    "\n",
    "for blk in blocks:\n",
    "    blk_orig = blk\n",
    "\n",
    "    # detectar ato\n",
    "    m_ato = re.match(rf\"^\\s*({verbs_regex})\\b\", blk, flags=re.IGNORECASE)\n",
    "    if not m_ato:\n",
    "        continue\n",
    "\n",
    "    ato = m_ato.group(1).upper()\n",
    "    content = blk[len(m_ato.group(0)):].strip()\n",
    "\n",
    "    subblocks = re.split(\n",
    "        r\"(?<=\\.)\\s+|(?<=;)\\s+|(?=(?:\\bNOMEAR\\b|\\bEXONERAR\\b|\\bDESIGNAR\\b|\\bEXONERAR A PEDIDO\\b))\",\n",
    "        content,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    if not subblocks:\n",
    "        subblocks = [content]\n",
    "\n",
    "    for sb in subblocks:\n",
    "        s = sb.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "\n",
    "        s_clean = re.sub(r\"(?i)\\bpor\\s+estar\\s+sendo\\b[^\\.,]*[\\,]?\", \"\", s)\n",
    "        s_clean = s_clean.strip().lstrip(\",\").strip()\n",
    "\n",
    "        names = name_re.findall(s_clean)\n",
    "\n",
    "        if not names:\n",
    "            m_after = re.match(\n",
    "                r\"^\\s*([A-ZÁÉÍÓÚÂÊÔÃÕÇ][A-ZÁÉÍÓÚÂÊÔÃÕÇ\\.\\-']+(?:\\s+[A-ZÁÉÍÓÚÂÊÔÃÕÇ\\.\\-']+){1,6})\",\n",
    "                s_clean\n",
    "            )\n",
    "            if m_after:\n",
    "                names = [m_after.group(1)]\n",
    "\n",
    "        if not names:\n",
    "            continue\n",
    "\n",
    "        # -----------------------------\n",
    "        # Loop de múltiplos nomes\n",
    "        # -----------------------------\n",
    "        for nm in names:\n",
    "\n",
    "            pos = s_clean.find(nm)\n",
    "            if pos == -1:\n",
    "                continue\n",
    "\n",
    "            remaining = s_clean[pos + len(nm):].strip()\n",
    "\n",
    "            matricula = None\n",
    "            sigrh = None\n",
    "            simbolos = []\n",
    "            cargo = None\n",
    "            orgao = None\n",
    "\n",
    "            mm = mat_re.search(remaining)\n",
    "            if mm:\n",
    "                matricula = mm.group(1).strip()\n",
    "\n",
    "            ms = sigrh_re.findall(remaining)\n",
    "            if ms:\n",
    "                sigrh = ms[0] if ato.startswith(\"EXONERAR\") else ms[-1]\n",
    "\n",
    "            sims = sim_re.findall(remaining)\n",
    "            if sims:\n",
    "                simbolos = [re.sub(r\"[\\s]+\",\"-\",x).upper() for x in sims]\n",
    "\n",
    "            # -----------------------------\n",
    "            # CARGO — TODAS AS REGEX CORRIGIDAS\n",
    "            # -----------------------------\n",
    "\n",
    "            mc = re.search(\n",
    "                r\"para\\s+exercer\\s+o\\s+\"\n",
    "                r\"(?:Cargo(?: Público)?(?: em)?(?: de)?(?: Natureza Especial|[^,;\\.]+)\"\n",
    "                r\"|Cargo em Comissão|Cargo Público em Comissão)\"\n",
    "                r\"[\\s,]*(?:,|de)?\\s*([^,;\\.]+)\",\n",
    "                s_clean,\n",
    "                flags=re.IGNORECASE\n",
    "            )\n",
    "\n",
    "            if mc:\n",
    "                cargo = mc.group(1).strip().rstrip(\".,;\")\n",
    "            else:\n",
    "                mc2 = re.search(\n",
    "                    r\"do\\s+(Cargo(?: Público)?(?: em)?(?: de)?(?: Natureza Especial|[^,;\\.]+)\"\n",
    "                    r\"|Cargo em Comissão|Cargo Público em Comissão)\"\n",
    "                    r\"[\\s,]*(?:,|de)?\\s*([^,;\\.]+)\",\n",
    "                    s_clean,\n",
    "                    flags=re.IGNORECASE\n",
    "                )\n",
    "                if mc2:\n",
    "                    cargo = mc2.group(1).strip()\n",
    "                else:\n",
    "                    # REGEX corrigida — sem parênteses extra\n",
    "                    mr = re.search(\n",
    "                        r\"\\b(?:de|,)\\s*([A-ZÁÉÍÓÚÂÊÔÃÕÇ][A-Za-záéíóúâêôãõçº°]+\"\n",
    "                        r\"(?:\\s+[A-ZÁÉÍÓÚÂÊÔÃÕÇa-záéíóúâêôãõçº°]+){0,5})\",\n",
    "                        remaining\n",
    "                    )\n",
    "                    if mr:\n",
    "                        cand_role = mr.group(1).strip()\n",
    "                        if len(cand_role.split()) <= 5:\n",
    "                            cargo = cand_role\n",
    "\n",
    "            # -----------------------------\n",
    "            # ÓRGÃO\n",
    "            # -----------------------------\n",
    "            org_matches = org_re.findall(s_clean)\n",
    "            if org_matches:\n",
    "                last_org = org_matches[-1]\n",
    "                if isinstance(last_org, tuple):\n",
    "                    last_org = next((x for x in last_org if x), last_org[0])\n",
    "                orgao = re.sub(r\"^(da|do|das|dos)\\s+\", \"\", last_org.strip(), flags=re.IGNORECASE)\n",
    "\n",
    "            # fallback para cargo após SIGRH/símbolo\n",
    "            if not cargo:\n",
    "                m_after_sym = re.search(\n",
    "                    r\"(?:Símbolo[:\\s]*[A-Z0-9\\-\\s]+[,;\\s]*)?\"\n",
    "                    r\"(?:SIGRH[:\\s]*\\d+[,;\\s]*)?(.*)$\",\n",
    "                    remaining\n",
    "                )\n",
    "                if m_after_sym:\n",
    "                    maybe = m_after_sym.group(1).strip()\n",
    "                    maybe_piece = maybe.split(\",\")[0].strip()\n",
    "                    if len(maybe_piece.split()) <= 6 and len(maybe_piece) > 3:\n",
    "                        if re.search(r\"[A-Za-zÀ-ÿ]\", maybe_piece):\n",
    "                            cargo = maybe_piece\n",
    "\n",
    "            clean_name = nm.strip()\n",
    "            if re.match(rf\"(?i)^(?:{verbs_regex})\\b\", clean_name):\n",
    "                clean_name = re.sub(rf\"(?i)^(?:{verbs_regex})\\b\\.?,?\\s*\", \"\", clean_name).strip()\n",
    "            clean_name = title_case_name(clean_name)\n",
    "\n",
    "            simbolo_main = None\n",
    "            if simbolos:\n",
    "                simbolos_norm = [re.sub(r\"[^\\w\\-]\",\"\",s).upper() for s in simbolos]\n",
    "                simbolo_main = simbolos_norm[-1] if ato.startswith(\"NOMEAR\") else simbolos_norm[0]\n",
    "\n",
    "            entry = {\n",
    "                \"uid\": None,\n",
    "                \"secao\": \"SEÇÃO II\",\n",
    "                \"ato\": ato,\n",
    "                \"nome\": clean_name,\n",
    "                \"matricula\": matricula,\n",
    "                \"sigrh\": sigrh,\n",
    "                \"simbolo\": simbolo_main,\n",
    "                \"cargo\": (cargo.strip() if cargo else None),\n",
    "                \"orgao\": (orgao.strip() if orgao else None),\n",
    "                \"raw\": s_clean\n",
    "            }\n",
    "\n",
    "            uid_base = f\"{ato}-{matricula or clean_name.replace(' ','_')}\"\n",
    "            entry[\"uid\"] = re.sub(r\"[^\\w\\-_]\",\"\", uid_base)[:80]\n",
    "\n",
    "            entries.append(entry)\n",
    "\n",
    "# deduplicação\n",
    "seen = set()\n",
    "unique = []\n",
    "for e in entries:\n",
    "    key = (e.get(\"ato\"), e.get(\"nome\"), e.get(\"matricula\"))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    unique.append(e)\n",
    "\n",
    "entries = unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe84ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(entries, ensure_ascii=False, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
